{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project 1: Implementing Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import copy as copy\n",
    "import timeit\n",
    "from sklearn import tree as sk_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity_calculator class\n",
    "\n",
    "Contains all the methods used to calculate the impurity of a feature. The calculator object is given an array of the values of the feature it is calculating impurity for, and an array with the corresponding labels. It is also given an impurity measure, which can be 'entropy' or 'gini'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Impurity_calculator:\n",
    "    def __init__(self, feature_column, y_values, impurity_measure):\n",
    "        \"\"\"\n",
    "        Class used to calculate the impurity value of a feature. \n",
    "        This allows the algorithm to find the best splitting feature.\n",
    "        \n",
    "        :param feature_column: array of values for the feature we want to find the impurity value of\n",
    "        :param y_values: array of labels corresponding to the feature values\n",
    "        :param impurity_measure: either 'gini' or 'entropy'. Determines how impurity is measured by the calcuator.\n",
    "        \"\"\"\n",
    "        self.feature_column = feature_column\n",
    "        self.y = y_values\n",
    "        self.impurity_measure = impurity_measure\n",
    "\n",
    "    def prob_feat_relation_to_bound(self, feature_is_less_than):\n",
    "        \"\"\"\n",
    "        P(feature_column[i] < / >= mean(feature_column))\n",
    "        Calculates the probability of a feature being less than the mean,\n",
    "        or the probability of it being greater than or equal to the mean.\n",
    "\n",
    "        :param feature_is_less_than: True if we are looking for the probability that a feature\n",
    "        value is less than the mean of the feature, False otherwise\n",
    "\n",
    "        :return : probability of feature\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        for data_point in self.feature_column:\n",
    "            if feature_is_less_than and data_point < np.mean(self.feature_column):\n",
    "                    counter += 1\n",
    "            elif not(feature_is_less_than) and data_point >= np.mean(self.feature_column):\n",
    "                    counter += 1\n",
    "        return counter/(len(self.feature_column))\n",
    "\n",
    "    def prob_true_given_conditional(self, feature_is_less_than):\n",
    "        \"\"\"\n",
    "        Calculates the probability of a data point in the feature being less than / greater or equal to its mean.\n",
    "\n",
    "        :param feature_less_than_bound: boolean which is True if the the relation of the feature to the \n",
    "        bound is \"less than\". False is it is \"greater than or equal to\"\n",
    "        :return : proability of a data point in the feature being smaller / larger than its mean.\n",
    "        \"\"\"\n",
    "        total_counter = 0\n",
    "        true_counter = 0\n",
    "        for i in range(len(self.feature_column)):\n",
    "            if not(feature_is_less_than) and (self.feature_column[i] >= np.mean(self.feature_column)):\n",
    "                total_counter +=1\n",
    "                if (self.y[i] == 1):\n",
    "                    true_counter = true_counter + 1\n",
    "            elif feature_is_less_than and (self.feature_column[i] < np.mean(self.feature_column)):\n",
    "                total_counter += 1\n",
    "                if (self.y[i] == 1):\n",
    "                    true_counter = true_counter + 1\n",
    "        return true_counter / total_counter    \n",
    "\n",
    "    def entpy_of_feat_in_relation_to_bound(self, feature_less_than_bound):\n",
    "        \"\"\"\n",
    "        Calculates conditional entropy, so the entropy of a feature given that it is less / greater or equal to a bound (the mean).\n",
    "        \n",
    "        :param feature_less_than_bound: boolean which is True if the the relation of the feature to the \n",
    "        bound is \"less than\". False is it is \"greater than or equal to\"\n",
    "        :return : etnropy of feature given that it is greater /less than or equal to a bound.\n",
    "        \"\"\"\n",
    "        p_true = self.prob_true_given_conditional(feature_less_than_bound)\n",
    "        if p_true == 0:\n",
    "            pos_part = 0\n",
    "        else:\n",
    "            pos_part = -p_true* np.log2(p_true)\n",
    "        p_false = 1-p_true\n",
    "        if p_false == 0:\n",
    "            neg_part = 0\n",
    "        else:\n",
    "            neg_part = -p_false*np.log2(p_false)\n",
    "        h = pos_part + neg_part\n",
    "        return h\n",
    "\n",
    "    def gini_of_feat_in_relation_to_bound(self, feature_less_than_bound):\n",
    "        \"\"\"\n",
    "        Calculated the conditional gini, ie the gini of a feature given that it is less / greater or equal to a bound (the mean).\n",
    "\n",
    "        :param feature_column: array of values for the feature\n",
    "        :param y: array of corresponding labels\n",
    "        :param feature_less_than_bound: boolean which is True if the the relation of the feature to the \n",
    "        bound is \"less than\". False is it is \"greater than or equal to\"\n",
    "        :return : gini of a feature given that it is less / greater or equal to a bound\n",
    "        \"\"\"\n",
    "        p_true = self.prob_true_given_conditional(feature_less_than_bound)\n",
    "        p_false = 1-p_true\n",
    "        return 1 - (p_true**2 + p_false**2)\n",
    "\n",
    "    def gini_of_feature(self):\n",
    "        \"\"\"\n",
    "        Calculates the gini index of the feature of this calculator.\n",
    "        \n",
    "        :return : gini index of the feature\n",
    "        \"\"\"\n",
    "        neg = self.prob_feat_relation_to_bound(True) * self.gini_of_feat_in_relation_to_bound(True)\n",
    "        pos = self.prob_feat_relation_to_bound(False) * self.gini_of_feat_in_relation_to_bound(False)\n",
    "        return neg + pos\n",
    "\n",
    "    def entpy_of_feature(self):\n",
    "        \"\"\"\n",
    "        Calculates the entropy of a feature\n",
    "\n",
    "        :return : entropy of feature\n",
    "        \"\"\"\n",
    "        neg = self.prob_feat_relation_to_bound(True) * self.entpy_of_feat_in_relation_to_bound(True)\n",
    "        pos = self.prob_feat_relation_to_bound(False) * self.entpy_of_feat_in_relation_to_bound(False)\n",
    "        return neg + pos\n",
    "\n",
    "    def impurity_of_feature(self):\n",
    "        \"\"\"\n",
    "        Calculates the impurity of a feature, based on the impurity measure the calculator is given.\n",
    "\n",
    "        :return : impurity measure of feature given calcuator object.\n",
    "        \"\"\"\n",
    "        if self.impurity_measure == 'entropy':\n",
    "            return self.entpy_of_feature()\n",
    "        elif self.impurity_measure == 'gini':\n",
    "            return self.gini_of_feature()\n",
    "        else:\n",
    "            print(\"Error: invalid impurity measure given:\", self.impurity_measure)\n",
    "            return    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class\n",
    "\n",
    "The node object is used by the DecisionTree class to create a tree in the form of a linked-list-type datastucture of Node object. Each node (apart from the root) has a parent in the form of a Node object, and may have two children in the form of new Node objects. If a Node has no children it is assigned a leaf value, which in this case is either a 0 or a 1 int.\n",
    "\n",
    "Additionally, the index of the feature it is splitting on, and the data it is splitting is stored in the node. This makes recursive pruning possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index, name, X, y, left = None, right = None, leaf = None):\n",
    "        \"\"\"\n",
    "        Represents a node used in a decision tree. Used by the DecisionTree class.\n",
    "        \n",
    "        :param feature_index: int representing the index of the column of the feature that this node is splitting on\n",
    "        :param name: string representing the node. Used for printing.\n",
    "        :param X: 2d array of feature values that are to be split by this node.\n",
    "        :param y: array of labels corresponding to the feature values.\n",
    "        :param left: left child of node\n",
    "        :param right: right child of node\n",
    "        :param leaf: int representing the leaf value of the node\n",
    "        \"\"\"\n",
    "        self.feature_index = feature_index\n",
    "        self.name = name\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.leaf = leaf\n",
    "        self.X = X #keep track of the X-values used for calculating entropy for later pruning\n",
    "        self.y = y #used for pruning\n",
    "        self.feature_mean = np.mean(X[:,self.feature_index])\n",
    "        self.majority_label = np.bincount(y.astype(int)).argmax() # used for pruning\n",
    "        \n",
    "        \n",
    "    def is_pure(self, y_values):\n",
    "        \"\"\"\n",
    "        Checks if a node is pure, ie if all the training data at this point in the tree creation process \n",
    "        has the same y-values.\n",
    "        \n",
    "        :param y values: y values of the training data potentially use to expand the tree at this node\n",
    "        :return : pure value, boolean representing if node is pure\n",
    "        \"\"\"\n",
    "        if (all(y == 0 for y in y_values)):\n",
    "            return 0, True\n",
    "        elif (all(y == 1 for y in y_values)):\n",
    "            return 1, True\n",
    "        return 0, False\n",
    "    \n",
    "    def accuracy_majority_label(self, y_unseen):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of using the majority label of the node to predict the y values in the pruning data.\n",
    "        \n",
    "        :param y_unseen: the y values used to test the accuracy of using the majority label to predict\n",
    "        :return : accuracy of the majority label\n",
    "        \"\"\"\n",
    "        corretly_labeled_count = 0\n",
    "        for i in range (len(y_unseen)):\n",
    "            if (y_unseen[i] == self.majority_label):\n",
    "                corretly_labeled_count += 1\n",
    "        return corretly_labeled_count/len(y_unseen)\n",
    "    \n",
    "    def print_tree_from_node(self):\n",
    "        \"\"\"\n",
    "        Used for testing. Recursively prints a tree from the node this function was first called on.\n",
    "        So, if this function is called on the root of a tree, the entire tree is printed.\n",
    "        \"\"\"\n",
    "        if (self.name == \"empty tree\"):\n",
    "            print(self.name)\n",
    "            return\n",
    "        if ((self.left is None) and (self.right is None)):\n",
    "            print(\"⟨\" +  str(self.leaf) + \"⟩\", end =\"\")\n",
    "            return\n",
    "        else:\n",
    "            print(self.name, end = \": \")\n",
    "        print('(', end = \"\")\n",
    "        if (self.left is not None):\n",
    "             self.left.print_tree_from_node()\n",
    "        print(') (', end = \"\")\n",
    "        if (self.right is not None):\n",
    "            self.right.print_tree_from_node()\n",
    "        print(' )', end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree class\n",
    "\n",
    "This class is initialized only with the training data it recieves. The *learn* method takes in *impurity_measure* and *prune_tree* as parameters, which determine how the decision tree is generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        self.total_number_of_features = len(self.X[0])\n",
    "        self.root = Node(0, \"empty tree\", X_train, y_train)\n",
    "    \n",
    "    \n",
    "    def learn(self, impurity_measure, prune_tree):\n",
    "        \"\"\"\n",
    "        Calls on id3 with the appropriate parameters, given those it recieves. \n",
    "        If the tree should be pruned, training data is split into training and pruning data.\n",
    "        \n",
    "        :param impurity_measure: 'entropy' or 'gini'. Determines how the features are split.\n",
    "        :param prune_tree: boolean which is True if the tree should be pruned, False otherwise\n",
    "        \"\"\"\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.prune_tree = prune_tree\n",
    "        used_features = []\n",
    "        if prune_tree:\n",
    "            X_train, X_prune, y_train, y_prune = train_test_split(self.X, self.y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=seed)\n",
    "            self.root = self.id3(X_train, y_train, X_prune, y_prune, used_features)\n",
    "        else:\n",
    "            self.root = self.id3(self.X, self.y, [], [], used_features)\n",
    "\n",
    "    \n",
    "    \n",
    "    def id3(self, X, y, X_prune, y_prune, used_features):\n",
    "        \"\"\"\n",
    "        This method peforms a version of the id3 algorithm described in the project description, with some additions.\n",
    "        \n",
    "        :param X: feature values used to create the tree\n",
    "        :param y: label values used to create the tree\n",
    "        :param X_prune: feature values used to prune\n",
    "        :param y_prune: label values used to prune \n",
    "        :param used_features: list of the indexes of features that have already been split on\n",
    "        :return current_node: node that is a root of a fully built tree. This is in most cases a subtree.\n",
    "        \"\"\"\n",
    "        current_node = Node(0, \"unfinished node\" ,X = X, y = y)\n",
    "        label, pure = current_node.is_pure(y)\n",
    "        if pure:\n",
    "            current_node.leaf = label\n",
    "            return current_node\n",
    "        elif (self.all_feature_values_are_the_same(X)):\n",
    "            current_node.leaf = current_node.majority_label\n",
    "            return current_node\n",
    "        elif (len(used_features) >= self.total_number_of_features): #no more features to split\n",
    "            leaf_node = Node(0, \"leaf\", X = X, y = y) #initiate a leaf node\n",
    "            leaf_node.leaf = leaf_node.majority_label\n",
    "            return leaf_node\n",
    "        else: #split node, if possible\n",
    "            best_splitting_feature_index = self.get_best_splitting_feature_index(X, y, used_features)\n",
    "            current_node = Node(best_splitting_feature_index, \"X\"+str(best_splitting_feature_index),X = X, y = y)\n",
    "            neg_X, neg_y, pos_X, pos_y = self.split_data(current_node, X, y)\n",
    "            data_is_unsplittable, end_node = self.check_for_unsplittable_data(neg_X, pos_X, current_node)\n",
    "            if (data_is_unsplittable): return end_node #check if data was split properly\n",
    "            left_child, right_child = self.generate_children(current_node, neg_X, neg_y, pos_X, pos_y, X_prune, y_prune, used_features)\n",
    "            #if the generated children aren't identical leaves, assignm them to the current node\n",
    "            if (left_child.leaf == None or (left_child.leaf != right_child.leaf)):\n",
    "                current_node.left = left_child\n",
    "                current_node.right = right_child\n",
    "            elif (left_child.leaf != None and (left_child.leaf == right_child.leaf)): #if both children are identical leaves, node becomes a leaf\n",
    "                current_node.leaf = self.most_common_label(y)\n",
    "                current_node.name = 'leaf'\n",
    "        if self.prune_tree:\n",
    "            self.prune_from_node(current_node, X_prune, y_prune)\n",
    "        return current_node\n",
    "    \n",
    "    def generate_children(self, parent_node, neg_X, neg_y, pos_X, pos_y, X_prune, y_prune, used_features):\n",
    "        \"\"\"\n",
    "        Prepares data needed to create left and right child node, and returns the child nodes.\n",
    "        \n",
    "        :param parent_node: node that the children are generated from\n",
    "        :param neg_X, neg_y: training data to be given to the left child\n",
    "        :param pos_X, pos_y: training data to be given to the right child\n",
    "        :param X_prune, y_prune: prune data to be split and passed on to children\n",
    "        :param used_features: list of features that have been split on\n",
    "        :return left_child, right_child: children of the parent_node\n",
    "        \"\"\"\n",
    "        neg_X_prune, neg_y_prune, pos_X_prune, pos_y_prune = self.split_data(parent_node, X_prune, y_prune)\n",
    "        #deepcopy so as to not affect the used_features list in other parts of the tree\n",
    "        left_used_features = copy.deepcopy(used_features)\n",
    "        right_used_features = copy.deepcopy(used_features)\n",
    "        left_child = self.id3(neg_X, neg_y, neg_X_prune, neg_y_prune, left_used_features)\n",
    "        right_child = self.id3(pos_X, pos_y, pos_X_prune, pos_y_prune, right_used_features)\n",
    "        return left_child, right_child\n",
    "    \n",
    "    def check_for_unsplittable_data(self, neg_X, pos_X, current_node):\n",
    "        \"\"\"\n",
    "        Checks if when splitting the data, all the data ended up on one side. Ie, the data was not split.\n",
    "        Since we split by the mean, this should never happen.\n",
    "        \n",
    "        :param neg_X: data split to the left\n",
    "        :param pos_X: data split to the right\n",
    "        :param current_node: node we are splitting on\n",
    "        :return : True if data is unsplittable, node to return if data is not splittable\n",
    "        \"\"\"\n",
    "        if len(neg_X) == 0 or len(pos_X) == 0:\n",
    "            current_node.leaf = self.most_common_label(y)\n",
    "            current_node.name = 'leaf'\n",
    "            print(\"Data cannot be split by the mean\", current_node.feature_mean,\", so node X\", current_node.feature_index, \" becomes leaf with label\", current_node.leaf, \"\\n\")\n",
    "            print(\"ERROR: this should never happen, since if we split by the mean there is always data on each side of the split\")\n",
    "            return True, current_node\n",
    "        return False, None\n",
    "    \n",
    "    def prune_from_node(self, current_node, X_prune, y_prune):\n",
    "        \"\"\"\n",
    "        If the accuracy of the majority label on this node is better than the predictions of the subtree,\n",
    "        the children of the node are removes, and the node becomes a leaf.\n",
    "        \n",
    "        :param current_node: node that might be pruned\n",
    "        :param X_prune: feature values for pruning data\n",
    "        :param y_prune: label values for pruning data\n",
    "        \"\"\"\n",
    "        if current_node.leaf != None: # If current node already is a leaf, there is nothing to do.\n",
    "            return\n",
    "        if (current_node.accuracy_majority_label(y_prune) >= (self.accuracy_full_tree_from_node(X_prune, y_prune, current_node))):\n",
    "            current_node.left = None\n",
    "            current_node.right = None\n",
    "            current_node.leaf = current_node.majority_label\n",
    "            \n",
    "    def split_data(self, current_node, X, y):\n",
    "        \"\"\"\n",
    "        Splits data by whether the value of a certian feature of less than the mean of the feature, or not.\n",
    "        \n",
    "        :param current_node: node on which the data will be split\n",
    "        :param X: feature data\n",
    "        :param y: label data\n",
    "        :return : \"neg_X\" and \"neg_y\" contain all rows where the feature in question was less than the mean,\n",
    "        while \"pos_X\" and \"pos_y\" contain the rest of the rows.\n",
    "        \"\"\"\n",
    "        if ((len(X) == 0) or (len(y) == 0)): #nothing to split\n",
    "            return [], [], [], []\n",
    "        feature_index = current_node.feature_index\n",
    "        new_X = copy.deepcopy(X) # make deepcopy to avoid affecting the data of other nodes\n",
    "        new_y = copy.deepcopy(y)\n",
    "        merged_data = np.insert(new_X, len(new_X[0]), new_y, axis=1)\n",
    "        # mean of the features is taken from the node\n",
    "        #as it containes the subset of the total dataset from which the mean at this split should be calculated\n",
    "        mean = current_node.feature_mean \n",
    "        neg_merged = merged_data[np.where(merged_data[:, feature_index] < mean)]\n",
    "        pos_merged = merged_data[np.where(merged_data[:, feature_index] >= mean)]\n",
    "        neg_X = neg_merged[:,:-1]\n",
    "        neg_y = neg_merged[:, -1]\n",
    "        pos_X = pos_merged[:,:-1]\n",
    "        pos_y = pos_merged[:, -1]\n",
    "        return neg_X, neg_y, pos_X, pos_y\n",
    "        \n",
    "            \n",
    "    def get_best_splitting_feature_index(self, X, y, used_features):\n",
    "        \"\"\"\n",
    "        Returns the index of the feature that has the highest information gain. Ie, lowest entropy or gini.\n",
    "        \n",
    "        :param X: feature data\n",
    "        :param y: label data\n",
    "        :param used_features: list of indexes that already have been split on.\n",
    "        :return : index of feature with least impurity, aka highest information gain\n",
    "        \"\"\"\n",
    "        min_impurity = None\n",
    "        best_feature = None\n",
    "        for i in range(len(X[0])):\n",
    "            if(i in used_features): #if the feature already has been used to split, skip it\n",
    "                continue\n",
    "            feature_column = np.array(X)[:,i]\n",
    "            calculator = Impurity_calculator(feature_column, y, self.impurity_measure)\n",
    "            impurity_of_feature_with_index_i = calculator.impurity_of_feature()\n",
    "            if ((min_impurity is None) or (min_impurity > impurity_of_feature_with_index_i)):\n",
    "                    min_impurity = impurity_of_feature_with_index_i\n",
    "                    best_feature = i    \n",
    "        used_features.append(best_feature)\n",
    "        return best_feature\n",
    "    \n",
    "    def most_common_label(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the most common label in the label-array. \n",
    "        Only takes the labels True and False (ie 1 and 0) into account.\n",
    "        \n",
    "        :param Y: array of y-values\n",
    "        :return : most common value in y\n",
    "        \"\"\"\n",
    "        true_count = 0\n",
    "        false_count = 0\n",
    "        for y in Y:\n",
    "            if y == 0:\n",
    "                false_count += 1\n",
    "            else:\n",
    "                true_count += 1\n",
    "        if (false_count > true_count):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def predict(self, new_data_point, current_node = None):\n",
    "        \"\"\"\n",
    "        Predicts the label of a given new data point. \n",
    "        This is done recursively, passing the next node to be searched from in the parameter.\n",
    "        If no current node is given, the default is to start searching from the root.\n",
    "        \n",
    "        :param new_data_point: array of feature values\n",
    "        :param current node: node to search from.\n",
    "        :return : the predicted y value of the given x-values\n",
    "        \"\"\"\n",
    "        if (len(new_data_point) != len(self.X[0])):\n",
    "            print(\"invalid data point to predict\")\n",
    "            return\n",
    "        elif (current_node is None):\n",
    "            current_node = self.root\n",
    "        if (current_node.left == None): # the right child will be None too\n",
    "            return current_node.leaf # this is a leaf node\n",
    "        else:\n",
    "            mean = current_node.feature_mean\n",
    "            if (new_data_point[current_node.feature_index] < mean):\n",
    "                return self.predict(new_data_point, current_node.left)\n",
    "            else:\n",
    "                return self.predict(new_data_point, current_node.right)\n",
    "            \n",
    "            \n",
    "    def accuracy_full_tree_from_node(self, X_unseen, y_unseen, current_node):\n",
    "        \"\"\"\n",
    "        Extracts all the elements in the column of the 2d array X\n",
    "\n",
    "        :param X_unseen: new feature data to used to predict values\n",
    "        :param y_unseen: new labels to compare predictions to\n",
    "        :param current_node: node that it is being predicted from\n",
    "        :return :\n",
    "        \"\"\"\n",
    "        corretly_labeled_count = 0\n",
    "        for i in range (len(y_unseen)):\n",
    "            if (self.predict(X_unseen[i], current_node) == y_unseen[i]):\n",
    "                corretly_labeled_count += 1\n",
    "        return corretly_labeled_count/len(y_unseen)\n",
    "    \n",
    "    def all_feature_values_are_the_same(self, X):\n",
    "        \"\"\"\n",
    "        Checks if all rows of feature values are the same.\n",
    "        \n",
    "        :param X: 2d array of feature values\n",
    "        :return : True of all feature values are the same, False otherwise\n",
    "        \"\"\"\n",
    "        if (len(X) == 1):\n",
    "            return True\n",
    "        for i in range(len(X)-1):\n",
    "            if not(np.array_equal(X[i], X[i+1])):\n",
    "                return False\n",
    "        print(\"All features are the same\")\n",
    "        return True        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm evaluation\n",
    "\n",
    "We load in the dataset, and split it into training, valuation and testing sets.\n",
    "\n",
    "Then we create our decision tree object, and try our the four different hyperparameter combinations of *entropy*/*gini* and *pruning*/*no pruning*. We evaluate each version with the valuation dataset, using the *accuracy_full_tree_from_node* method, and calling it with the root of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "entropy + no prune\n",
      "X0: (X1: (⟨1⟩) (X2: (⟨1⟩) (⟨0⟩ ) )) (X2: (X1: (X3: (⟨1⟩) (⟨0⟩ )) (⟨0⟩ )) (⟨0⟩ ) )\n",
      "Validation Accuracy: 0.8394160583941606\n",
      "\n",
      "\n",
      "gini + no prune\n",
      "X0: (X1: (⟨1⟩) (X2: (⟨1⟩) (⟨0⟩ ) )) (X2: (X1: (X3: (⟨1⟩) (⟨0⟩ )) (⟨0⟩ )) (⟨0⟩ ) )\n",
      "Validation Accuracy: 0.8394160583941606\n",
      "\n",
      "\n",
      "entropy + prune\n",
      "X0: (⟨1⟩) (X2: (X1: (X3: (⟨1⟩) (⟨0⟩ )) (⟨0⟩ )) (⟨0⟩ ) )\n",
      "Validation Accuracy: 0.8467153284671532\n",
      "\n",
      "\n",
      "gini + prune\n",
      "X0: (⟨1⟩) (X2: (X1: (X3: (⟨1⟩) (⟨0⟩ )) (⟨0⟩ )) (⟨0⟩ ) )\n",
      "Validation Accuracy: 0.8467153284671532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = np.genfromtxt('data_banknote_authentication.txt', delimiter = ',')\n",
    "X_data = data[:,:-1]\n",
    "y_data = data[:,-1]\n",
    "\n",
    "#split data\n",
    "seed = 123\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_data, y_data,\n",
    "                                       test_size = 0.2,\n",
    "                                       shuffle = True, \n",
    "                                       random_state = seed)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                        test_size=0.5,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=seed)\n",
    "\n",
    "#Train tree with different combinations, and print accuracies\n",
    "my_tree = DecisionTree(X_train, y_train)\n",
    "my_tree.learn('entropy', False)\n",
    "print(\"\\n\\nentropy + no prune\")\n",
    "my_tree.root.print_tree_from_node()\n",
    "print(\"\\nValidation Accuracy:\", my_tree.accuracy_full_tree_from_node(X_val, y_val, my_tree.root))\n",
    "\n",
    "my_tree.learn('gini', False)\n",
    "print(\"\\n\\ngini + no prune\")\n",
    "my_tree.root.print_tree_from_node()\n",
    "print(\"\\nValidation Accuracy:\", my_tree.accuracy_full_tree_from_node(X_val, y_val, my_tree.root))\n",
    "\n",
    "my_tree.learn('entropy', True)\n",
    "print(\"\\n\\nentropy + prune\")\n",
    "my_tree.root.print_tree_from_node()\n",
    "print(\"\\nValidation Accuracy:\", my_tree.accuracy_full_tree_from_node(X_val, y_val, my_tree.root))\n",
    "\n",
    "my_tree.learn('gini', True)\n",
    "print(\"\\n\\ngini + prune\")\n",
    "my_tree.root.print_tree_from_node()\n",
    "print(\"\\nValidation Accuracy:\", my_tree.accuracy_full_tree_from_node(X_val, y_val, my_tree.root))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there was no difference in performance when gini or entropy were used as impurity measure, as the two split the trees identically.\n",
    "The addition of pruning only made a slight difference, improving the accurcy by less than a percent. Since the data sets contain a random sample, it can happen that the validation  data by chance favours some model over the other. So pruning does not always improve validation accuracy.\n",
    "\n",
    "We pick \"entropy + prune\" as our selected model, though by these results we could equally well have picked \"gini + prune\".\n",
    "If we wanted to make a more informed model selection, we could have done it with K-fold cross-validation.\n",
    "\n",
    "Now we assess the final model on unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy + prune\n",
      "X0: (⟨1⟩) (X2: (X1: (X3: (⟨1⟩) (⟨0⟩ )) (⟨0⟩ )) (⟨0⟩ ) )\n",
      "Accuracy on unseen testing data: 0.8913043478260869\n"
     ]
    }
   ],
   "source": [
    "my_tree.learn('entropy', True)\n",
    "print(\"entropy + prune\")\n",
    "my_tree.root.print_tree_from_node()\n",
    "my_tree_estimated_accuracy = my_tree.accuracy_full_tree_from_node(X_test, y_test, my_tree.root)\n",
    "print(\"\\nAccuracy on unseen testing data:\", my_tree_estimated_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my estimate for the performance of my algorithm on unseen data is about 89%, which isn't that great. If I had more time I'd like to figure out how to improve this value.\n",
    "It is however better than the accuracy on validation data, so we can be fairly certian that the selected model was a good choice.\n",
    "\n",
    "\n",
    "## Comparison to an existing implementation\n",
    "\n",
    "As suggested in the assignment, I am comparing my algorithm with the desicion tree implementation DecisionTreeClassifier from sklearn. It can use either gini, or entropy as inpurity measure, so I will pick the one which gives highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn with entropy accuracy: 1.0\n",
      "Sklearn with gini accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "sklearn_tree = sk_tree.DecisionTreeClassifier(criterion='entropy')\n",
    "sklearn_tree = sklearn_tree.fit(X_train, y_train)\n",
    "sk_predict = sklearn_tree.predict(X_val) \n",
    "sk_learn_accuracy = accuracy_score(y_val, sk_predict, normalize=True)\n",
    "print(\"Sklearn with entropy accuracy:\", sk_learn_accuracy)\n",
    "\n",
    "sklearn_tree = sk_tree.DecisionTreeClassifier(criterion='gini')\n",
    "sklearn_tree = sklearn_tree.fit(X_train, y_train)\n",
    "sk_predict = sklearn_tree.predict(X_val) \n",
    "sk_learn_accuracy = accuracy_score(y_val, sk_predict, normalize=True)\n",
    "print(\"Sklearn with gini accuracy:\", sk_learn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that gini and entropy do equally well when used in sklearns DecisionTreeClassifier. We pick entopy to compare with my algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My decision tree learning time:  0.6259505999999995\n",
      "Sklearn decision tree with gini learning time:  0.006669499999999218\n",
      "My decision tree accuracy: 0.8913043478260869\n",
      "Sklearn accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate runtime of my algorithm learning\n",
    "start = timeit.default_timer()\n",
    "my_tree.learn('entropy', True)\n",
    "stop = timeit.default_timer()\n",
    "print('My decision tree learning time: ', stop - start)  \n",
    "\n",
    "#Calculate runtime of sklearn decision tree learning\n",
    "sklearn_tree = sk_tree.DecisionTreeClassifier(criterion='entropy')\n",
    "start = timeit.default_timer()\n",
    "sklearn_tree = sklearn_tree.fit(X_train, y_train)\n",
    "stop = timeit.default_timer()\n",
    "print('Sklearn decision tree with gini learning time: ', stop - start)  \n",
    "\n",
    "print(\"My decision tree accuracy:\", my_tree_estimated_accuracy)\n",
    "    \n",
    "#Calculate accuracy of sklearn classifier\n",
    "sk_predict = sklearn_tree.predict(X_test) \n",
    "sk_learn_accuracy = accuracy_score(y_test, sk_predict, normalize=True)\n",
    "print(\"Sklearn accuracy:\", sk_learn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn clearly beats my implementation in both runtime and accuracy. \n",
    "I have not spent time improving the runtime of my algorithm, but I have an idea of where to start. I am sure that the hard workers that made DecisionTreeClassifier took much more care in ensuring the algorithm had a good runtime.\n",
    "Unlike my algorithm, sklearn's does not prune it's decision trees.Unless a max depth parameter is set, an sklearn decision tree can potentially become very large on some datasets. While my algorith does not have a max-depth set, the pruning may prevent trees from being unnecessarily large.\n",
    "Sklearn's decision tree does however split on a feature more than once, while my decision tree only splits once. This may be an explanation for why my implementation has so low accuracy, and is a main point of improvement for my algorithm. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
